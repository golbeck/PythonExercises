import numpy as np
####################################################################################
####################################################################################
def grad_cost(theta,X,Y,eps):
    #computes the gradient with respect to the parameters (theta) of the logistic regression model
    #theta: np.array of parameters
    #X: np.array of inputs (each of the m rows is a separate observation)
    #Y: np.array of outpus
    #eps: regularization constant (set to zero if unregularized)
    
    #dimension of data
    #number of rows (observations)
    n=X.shape[0]   
    #number of columns (features + bias) 
    m=X.shape[1]
    #number of rows in the dependent variable
    n_Y=Y.shape[0]
    
    #check if X and Y have the same number of observations
    if(n!=n_Y):
        print "number of rows in X and Y are not the same"
        return -9999.
        
    #compute logistic function
    g=1/(1+np.exp(-np.dot(X,theta)))
    #only the non-bias features are present in regularization terms in the cost func
    theta_reg=np.copy(theta)
    theta_reg[0]=0.0
    #gradient with respect to theta
    J_grad=(np.dot(X.T,g-Y)+eps*theta_reg)/n
    return J_grad
    
####################################################################################
#################################################################################### 
def cost_fn(theta,X,Y,eps):
    #cost function for logistic regression
    #theta: np.array of parameters
    #X: np.array of inputs (each of the m rows is a separate observation)
    #Y: np.array of outpus
    #eps: regularization constant (set to zero if unregularized)
    #dimension of data
    #number of rows (observations)
    n=X.shape[0]   
    #number of columns (features + bias) 
    m=X.shape[1]
    #number of rows in the dependent variable
    n_Y=Y.shape[0]
    #check if X and Y have the same number of observations
    if(n!=n_Y):
        print "number of rows in X and Y are not the same"
        return -9999.
    
    #only the non-bias features are in the regularization terms in the cost func
    theta_reg=np.copy(theta)
    theta_reg[0]=0.0
    #compute logistic function
    g=1/(1+np.exp(-np.dot(X,theta)))
    #log likelihood func
    temp0=-np.log(g)*Y-np.log(1-g)*(1-Y)
    temp1=theta_reg**2
    J=(temp0.sum(axis=0)+0.5*eps*temp1.sum(axis=0))/n
    return J
    
    
    

####################################################################################
####################################################################################    
pwd_temp=%pwd
dir1='/home/golbeck/Workspace/PythonExercises/NeuralNets'
if pwd_temp!=dir1:
    os.chdir(dir1)
dir1=dir1+'/data' 
dat=np.loadtxt(dir1+'/ex2data1.txt',unpack=True,delimiter=',',dtype={'names': ('X1', 'X2', 'Y'),'formats': ('f4', 'f4', 'i4')})
n=len(dat)
Y=dat[n-1]
m=len(Y)
X=np.array([dat[i] for i in range(n-1)]).T
X=np.column_stack((np.ones(m),np.copy(X)))
    
    
    
    
    
